{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJdIG9xM1bfY"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) with a Large Language Model (LLM)\n",
        "\n",
        "This notebook shows a RAG implementation for a TinyLLaMA model. Some different RAG and query prompting techniques are used and compared, such as Multi-Query document retrieval, Hypothetical Documents (HyDE), adding contextual prompts to the query, and varying the amount of retrieved documents.\n",
        "\n",
        "At the end, a fine tuned LLM is used to do sentiment analysis on a few sentences, to highlight a different type of task LLMs can perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80_ao0-s1bfa"
      },
      "source": [
        "## Setting up the libraries and the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvdPS-PdfYl8",
        "outputId": "7715d6d5-2cb2-4aa0-cea5-9a6dc8b67ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, langchain_community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0.post1 httpx-sse-0.4.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers sentence-transformers langchain langchain_community faiss-cpu torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mARErFREfgSx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1_HzD3-1bfa"
      },
      "source": [
        "## Data Preprocessing and Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0YdKLkLfaRC",
        "outputId": "927d534f-44a4-4098-9e95-5466de957da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "798\n",
            "Q: What is supervised learning? A: Supervised learning is a machine learning paradigm where the algorithm learns from labeled training data, making predictions or decisions based on input-output pairs.\n",
            "torch.Size([798, 78])\n",
            "Number of chunks: 798\n",
            "(798, 384)\n"
          ]
        }
      ],
      "source": [
        "# Computer Science Question/Answer dataset: https://huggingface.co/datasets/August4293/CS_QA\n",
        "# Combine the question/answer pairs into documents\n",
        "cs_qa_raw = load_dataset(\"August4293/CS_QA\", split=\"train\")\n",
        "documents = [f\"Q: {pair['question']} A: {pair['answer']}\" for pair in cs_qa_raw]\n",
        "print(len(documents))\n",
        "\n",
        "# Generate tokens for the documents\n",
        "model_name = \"TinyLLaMA/TinyLLaMA-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokens = tokenizer(documents, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "print(tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True))\n",
        "print(tokens['input_ids'].shape)\n",
        "\n",
        "# Chunks the data before vectorizing and storing\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunked_docs = splitter.create_documents(documents)\n",
        "chunks = [doc.page_content for doc in chunked_docs]\n",
        "print(\"Number of chunks:\", len(chunks))\n",
        "\n",
        "# Making a vector store with faiss and also making the embeddings\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(chunks)\n",
        "print(embeddings.shape)\n",
        "\n",
        "# Create and save index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings.astype(np.float32))\n",
        "index_doc_map = {i: doc for i, doc in enumerate(chunked_docs)}\n",
        "faiss.write_index(index, \"csqa_index.faiss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGgD2uqJ1bfb"
      },
      "source": [
        "## Implementing RAG using LangChain for different queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bY6VZHGXYgj"
      },
      "source": [
        "The main components of RAG with LangChain are as follows:\n",
        "- Query Translation:\n",
        "  - Augment the query to be more useful in retrieval systems.\n",
        "  - Rephrase, breakdown, abstract, generate hypothetical documents\n",
        "- Indexing:\n",
        "  - Make use of the vector stores in their efficient storage of vector embeddings.\n",
        "  - Offline step to store documents for retrieval.\n",
        "  - Important due to the efficient retrieval of relevant documents that it provides through vector embeddings.\n",
        "- Retrieval:\n",
        "  - Retrieve data that is similar to the translated query based on it's embedding.\n",
        "  - Includes ranking relevance to grab the most relevant information to respond to the query.\n",
        "- Generation:\n",
        "  - Generate a response given the retrieved information and the query.\n",
        "  - Can also use the generated response to inform more response generation in a positive feedback loop to improve the response.\n",
        "\n",
        "Other components exist such as:\n",
        "- Routing:\n",
        "  - Involves deciding on which data stores to query for information given the translated prompt.\n",
        "- Query construction:\n",
        "  - Involves constructing queries for the chosen data stores involved with RAG, based on the translated prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4T-cediuGnC"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCwGuVR9bR9m"
      },
      "outputs": [],
      "source": [
        "# RAG pipeline\n",
        "def rag_response(query, index, embedding_model, llm_model, llm_tokenizer, index_to_doc_map, top_k=3):\n",
        "    \"\"\"\n",
        "    Generate a response using the RAG pattern.\n",
        "\n",
        "    Args:\n",
        "        query: The user's question\n",
        "        index: FAISS index\n",
        "        embedding_model: Model to create embeddings\n",
        "        llm_model: Language model for generation\n",
        "        llm_tokenizer: Tokenizer for the language model\n",
        "        index_to_doc_map: Mapping from index positions to document chunks\n",
        "        top_k: Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        response: The generated response\n",
        "        sources: The source documents used\n",
        "    \"\"\"\n",
        "    # Create query embedding\n",
        "    query_embedding = embedding_model.encode([query]).astype(np.float32)\n",
        "\n",
        "    # Get top k docs\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    retrieved_docs = [index_to_doc_map[idx] for idx in indices[0]]\n",
        "\n",
        "    # Generate context from retrieved documents\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Create prompt for the model\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant. Answer the question based only on the provided context.\n",
        "If you don't know the answer based on the context, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "<|user|>\n",
        "{query}\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    input_ids = llm_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(llm_model.device)\n",
        "\n",
        "    generation_config = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"do_sample\": True\n",
        "    }\n",
        "\n",
        "    # Generate the output\n",
        "    with torch.no_grad():\n",
        "        output = llm_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            **generation_config\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = llm_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the model's response\n",
        "    response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7bE0d93cT_u",
        "outputId": "e2c1dea9-f258-4734-d6fc-663e18166782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is reinforcement learning in machine learning?\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Reinforcement learning is a type of machine learning that is used to train a decision-making agent to learn from its experiences and make decisions based on the rewards it receives. The agent receives feedback in the form of rewards or penalties based on its actions. Reinforcement learning is commonly used in applications such as robotics, finance, and games.\n",
            "\n",
            "==================================================\n",
            "Question: What are some differences between reinforcement learning and supervised learning?\n",
            "\n",
            "Response: Here are some differences between reinforcement learning and supervised learning:\n",
            "\n",
            "1. Learning Goals: Reinforcement learning aims to learn the optimal policy for a given task, which is different from supervised learning that aims to learn a specific label for each input-output pair.\n",
            "\n",
            "2. Input-Output Pair: Reinforcement learning learns based on the current state, whereas supervised learning learns based on the current input-output pair.\n",
            "\n",
            "3. Strategy or Policy: Reinforcement learning focuses on the policy, while supervised learning focuses on the strategy or decision making.\n",
            "\n",
            "4. Input-Output Data: Reinforcement learning relies on labeled training data, while supervised learning relies on unlabeled data.\n",
            "\n",
            "5. Learning Rate: Reinforcement learning uses a learning rate that adjusts the weight of the parameters, while supervised learning uses a learning rate that adjusts the regularization parameter.\n",
            "\n",
            "6. Computational Complexity: Reinforcement learning is computationally intensive, while supervised learning is relatively computationally efficient.\n",
            "\n",
            "7. Applications: Reinforcement learning is used in games and complex decision-making tasks, while\n",
            "\n",
            "==================================================\n",
            "Question: What are some benefits of reinforcement learning over deep reinforcement learning?\n",
            "\n",
            "Response: There are several benefits of reinforcement learning over deep reinforcement learning:\n",
            "\n",
            "1. Faster convergence: Reinforcement learning algorithms typically converge much faster than deep reinforcement learning methods.\n",
            "\n",
            "2. Lower data requirements: Reinforcement learning requires less data than deep reinforcement learning methods, making it a more practical approach for smaller datasets.\n",
            "\n",
            "3. Better generalization: Reinforcement learning can generalize better to unseen situations than deep reinforcement learning methods.\n",
            "\n",
            "4. Better performance on complex tasks: Reinforcement learning can handle more complex tasks than deep reinforcement learning methods, making it an ideal choice for tasks like robotics and self-driving cars.\n",
            "\n",
            "5. Better scalability: Reinforcement learning methods can scale to larger datasets and more complex environments than deep reinforcement learning methods.\n",
            "\n",
            "6. Improved interpretability: Reinforcement learning methods can provide more detailed and interpretable results than deep reinforcement learning methods, making them more suitable for applications like finance and healthcare.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Relevant queries to the CompSci dataset\n",
        "queries = [\n",
        "    \"What is reinforcement learning in machine learning?\",\n",
        "    \"What are some differences between reinforcement learning and supervised learning?\",\n",
        "    \"What are some benefits of reinforcement learning over deep reinforcement learning?\"\n",
        "]\n",
        "\n",
        "for question in queries:\n",
        "    print(f\"Question: {question}\\n\")\n",
        "\n",
        "    response = rag_response(\n",
        "        query=question,\n",
        "        index=index,\n",
        "        embedding_model=embedding_model,\n",
        "        llm_model=model,\n",
        "        llm_tokenizer=tokenizer,\n",
        "        index_to_doc_map=index_doc_map,\n",
        "        top_k=5\n",
        "    )\n",
        "\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzIEoMpPgirG"
      },
      "source": [
        "The LLM was prompted with 3 queries. Under each query, the strengths and weaknesses of the response are analyzed.\n",
        "- What is reinforcement learning in machine learning?\n",
        "  - This is a question that closely matches similar questions and information in the CS/QA dataset that I am using to augment the LLM. The definition that the LLM provides has no clear flaws and gives a general description of reinforcement learning. This is likely because of the similarity between the query and the retrieved documents\n",
        "- What are some differences between reinforcement learning and supervised learning.\n",
        "  - The first point is that reinforcement learning learns an optimal policy for a given task, while supervised learning learns labels for input output pairs. This is true, but is quite vague. That being said, it correctly identified the difference in learning goals.\n",
        "  - It also identifies that reinforcement learning learns based off of state and supervised learning learns off of input output pairs. This is quite vague, and I would also say that reinforcement learning learns primarily off of the reward it receives based on actions it takes in a state. Given the shortness of the responses in the CS/QA dataset, the LLM is likely reaching as it simply does not have the information to appropriately answer the question. That being said, it is not completely incorrect.\n",
        "  - It then says some somewhat truths like that reinforcement learning focuses on policy whereas supervised learning focuses on strategy.\n",
        "  - It struggles and hallucinates a bit when comparing the 2 ML methods. It says that reinforcement learning uses labelled data and supervised learning uses unlabelled data, which is clearly wrong. This is the distinction between supervised learning and unsupervised learning. I believe it must have thought the query was similar to a document that explains the difference between supervised and unsupervised learning, and got confused there.\n",
        "- What are some benefits of reinforcement learning over deep reinforcement learning?\n",
        "  - To this query, the LLM had some valid points and some hallucinations. For example, it states that reinforcement learning is more interpretable, which is definitely true. It also states that reinforcement learning is better at complex tasks, which is quite a general statement, but it is historically true that RL methods that are specific to a task tend to perform better than general Deep RL methods. However, it also stated that reinforcement learning is more general than Deep RL, which is simply not true, as RL often needs task specific feature engineering to work properly.\n",
        "  - Overall, the LLM may have found some vague information on RL and Deep RL to compare, but it seems to be likely using information about Deep Learning in general and comparing that to reinforcement learning or to other types of machine learning mistakenly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez9jz43E1bfb"
      },
      "source": [
        "## Modify and evaluate the different components of RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee0-suD-uG9b"
      },
      "outputs": [],
      "source": [
        "# Multi-Query document retrieval:\n",
        "def multi_query_rag_response(query, index, embedding_model, llm_model, llm_tokenizer, index_to_doc_map, top_k=3):\n",
        "    query_embedding = embedding_model.encode([query]).astype(np.float32)\n",
        "\n",
        "    # Multi-query augmentation\n",
        "    # Generate some additional similar queries\n",
        "    queries = [query, f\"What does {query} mean?\", f\"Reword {query} in a computer science context\"]\n",
        "    all_embeddings = embedding_model.encode(queries).astype(np.float32)\n",
        "\n",
        "    # Retrieve documents based on the new queries\n",
        "    retrieved_indices = set()\n",
        "    for emb in all_embeddings:\n",
        "        _, idxs = index.search(emb.reshape(1, -1), top_k)\n",
        "        retrieved_indices.update(idxs[0])\n",
        "    retrieved_docs = [index_to_doc_map[idx] for idx in retrieved_indices]\n",
        "\n",
        "    # SAME AS BASELINE RAG vvvvvvvvvvvvvvvvvvvv\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant. Answer the question based only on the provided context.\n",
        "If you don't know the answer based on the context, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "<|user|>\n",
        "{query}\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    input_ids = llm_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(llm_model.device)\n",
        "\n",
        "    generation_config = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"do_sample\": True\n",
        "    }\n",
        "\n",
        "    # Generate the output\n",
        "    with torch.no_grad():\n",
        "        output = llm_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            **generation_config\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = llm_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the model's response\n",
        "    response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# HyDE RAG:\n",
        "def hypthetical_doc_rag_response(query, index, embedding_model, llm_model, llm_tokenizer, index_to_doc_map, top_k=3):\n",
        "    # Generates a hypothetical document using baseline RAG and then use that doc to search for other docs\n",
        "    hypo_doc_query = f\"Write a short explanation or summary for the following question: {query}\"\n",
        "    hypo_doc = rag_response(hypo_doc_query, index, embedding_model, llm_model, llm_tokenizer, index_to_doc_map, top_k)\n",
        "    query_embedding = embedding_model.encode([hypo_doc]).astype(np.float32)\n",
        "\n",
        "    # SAME AS BASELINE RAG vvvvvvvvvvvvvvvvvvvv\n",
        "    # Get top k docs\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    retrieved_docs = [index_to_doc_map[idx] for idx in indices[0]]\n",
        "\n",
        "    # Generate context from retrieved documents\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Create prompt for the model\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a helpful AI assistant. Answer the question based only on the provided context.\n",
        "If you don't know the answer based on the context, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "<|user|>\n",
        "{query}\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    input_ids = llm_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(llm_model.device)\n",
        "\n",
        "    generation_config = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"do_sample\": True\n",
        "    }\n",
        "\n",
        "    # Generate the output\n",
        "    with torch.no_grad():\n",
        "        output = llm_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            **generation_config\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = llm_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the model's response\n",
        "    response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti5sXxYUprc2",
        "outputId": "8f5f5254-1caa-4025-a966-f8a358fec537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is reinforcement learning in machine learning?\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-Query Response: Reinforcement learning is a type of machine learning that uses a feedback loop to learn and improve an agent's behavior in an environment. The agent receives feedback in the form of rewards or penalties based on its actions, which can help the agent make better decisions. Reinforcement learning is a subset of machine learning, and it is often used in domains where there are a lot of decision-making or control tasks that require a high level of automation or precision.\n",
            "\n",
            "HyDE Response: Reinforcement learning is a type of machine learning (ML) methodology that enables an ML model to learn from experience and make decisions based on the rewards or punishments associated with those decisions. In reinforcement learning, an agent takes actions in an environment to maximize a reward signal over time, which is then used to update its decision-making process. Reinforcement learning can be applied in various domains, such as navigation, control, and decision-making.\n",
            "\n",
            "==================================================\n",
            "Question: What are some differences between reinforcement learning and supervised learning?\n",
            "\n",
            "Multi-Query Response: There are several differences between reinforcement learning and supervised learning:\n",
            "\n",
            "1. Training: Reinforcement learning requires a human to provide feedback to the agent, while supervised learning requires the agent to learn from labeled training data.\n",
            "\n",
            "2. Learning: Reinforcement learning is a continuous process, while supervised learning is a finite-time learning process.\n",
            "\n",
            "3. Reward signal: Reinforcement learning uses a reward signal to guide the agent's behavior, while supervised learning uses a pre-defined set of labels to provide the reward signal.\n",
            "\n",
            "4. Learning rate: In reinforcement learning, the learning rate is adjusted based on the performance of the agent, while supervised learning uses a fixed learning rate.\n",
            "\n",
            "5. Number of iterations: Reinforcement learning requires a large number of iterations, while supervised learning is typically done with a limited number of iterations.\n",
            "\n",
            "6. Hyperparameters: Reinforcement learning may require hyperparameters such as the discount factor, the learning rate, and the reward function, while supervised learning does not require hyperparameters.\n",
            "\n",
            "7. Model complexity: In reinforcement learning, the model is often small and simple, while in supervised\n",
            "\n",
            "HyDE Response: 1. Learning goal: In reinforcement learning, the goal is to learn a policy or action-value function that maximizes the reward. In supervised learning, the goal is to learn a function that can make predictions or decisions based on input-output pairs.\n",
            "2. Learning method: Reinforcement learning is an off-policy algorithm that learns by interacting with the environment. In supervised learning, the algorithm learns by training on a labeled dataset.\n",
            "3. Inputs: In reinforcement learning, the inputs are the environment's state and actions. In supervised learning, the inputs are the training data and the output is a prediction or decision.\n",
            "4. Output: In reinforcement learning, the outputs are rewards. In supervised learning, the outputs are predictions or decisions.\n",
            "5. Learning rate: In reinforcement learning, the learning rate is typically a hyperparameter that controls the rate at which the algorithm updates its parameters. In supervised learning, the learning rate is a fixed value.\n",
            "6. Error metric: In reinforcement learning, the error metric is typically the difference between the predicted and actual output. In supervised learning, the error metric is typically the difference between\n",
            "\n",
            "==================================================\n",
            "Question: What are some benefits of reinforcement learning over deep reinforcement learning?\n",
            "\n",
            "Multi-Query Response: There are several benefits of reinforcement learning over deep reinforcement learning:\n",
            "\n",
            "1. Faster convergence: Reinforcement learning is faster to converge than deep reinforcement learning. This is because it learns to make decisions based on the environment, rather than being pre-programmed with a specific policy.\n",
            "\n",
            "2. Reduced data requirements: Deep reinforcement learning requires a large amount of high-quality training data to train the neural network. Reinforcement learning, on the other hand, can learn from a limited amount of training data.\n",
            "\n",
            "3. Improved generalization: Deep learning models are more accurate, but they can struggle with complex tasks and unseen data. Reinforcement learning models, on the other hand, are better at generalizing to new data and can adapt to new situations.\n",
            "\n",
            "4. Better performance: Reinforcement learning can outperform deep learning models in tasks like image and speech recognition, where the reward signal is more complex and the environment is more challenging.\n",
            "\n",
            "5. Reduced manual labor: Reinforcement learning can automatically learn features from data, handle complex patterns, and perform well on tasks like image and speech recognition without extensive manual feature engineering.\n",
            "\n",
            "HyDE Response: Benefits of Reinforcement Learning over Deep Reinforcement Learning:\n",
            "1. Repeatable Tasks: Reinforcement learning is capable of handling repeated tasks while deep reinforcement learning may not be able to handle them.\n",
            "2. Flexibility: Reinforcement learning can work with a wide range of tasks and environments, whereas deep reinforcement learning may be limited to specific domains.\n",
            "3. Faster Learning: Reinforcement learning can learn faster and with less data, whereas deep reinforcement learning may take longer to learn and require more data.\n",
            "4. Robustness: Reinforcement learning is more robust than deep reinforcement learning, as it can handle uncertainties and failures in the environment.\n",
            "5. Efficiency: Deep reinforcement learning is more efficient in handling complex and high-dimensional tasks, whereas reinforcement learning can handle tasks with limited resources and complexity.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "for question in queries:\n",
        "    print(f\"Question: {question}\\n\")\n",
        "\n",
        "    mq_response = multi_query_rag_response(\n",
        "        query=question,\n",
        "        index=index,\n",
        "        embedding_model=embedding_model,\n",
        "        llm_model=model,\n",
        "        llm_tokenizer=tokenizer,\n",
        "        index_to_doc_map=index_doc_map,\n",
        "        top_k=5\n",
        "    )\n",
        "\n",
        "    print(f\"Multi-Query Response: {mq_response}\\n\")\n",
        "\n",
        "    hyde_response = hypthetical_doc_rag_response(\n",
        "        query=question,\n",
        "        index=index,\n",
        "        embedding_model=embedding_model,\n",
        "        llm_model=model,\n",
        "        llm_tokenizer=tokenizer,\n",
        "        index_to_doc_map=index_doc_map,\n",
        "        top_k=5\n",
        "    )\n",
        "\n",
        "    print(f\"HyDE Response: {hyde_response}\\n\")\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0qm7afrlbu"
      },
      "source": [
        "The first thing to note while using Multi-Query document retrieval and Hypothetical Document Embeddings is that both seemed to perform at least as good as the baseline, and often were more specific and correct in their responses. For example, both MQ and HyDE go more in-depth in their description of RL, with MQ describing the feedback loop in RL and HyDE describing the agent's interactions quite well. Unfortunately, the models still had some hallucinations. I believe that this is more a limit of the dataset combined with the specificity of the questions than the methods themselves. The MQ method is able to gather 3x as many documents as the baseline or HyDE, so it typically is able to include more information than HyDE. That said, HyDE seems to have the highest quality information, which is likely due to the  hypothetical document to document similarity improving the retrieved documents for the LLM to contextualize a query. Finally, both MQ and HyDE hallucinated less in the more difficult final 2 questions than the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsSA0yf7s4HW",
        "outputId": "8d58b3ff-815b-4e73-8245-d1ef7d230555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is reinforcement learning in machine learning?\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Reinforcement learning (RL) in machine learning is a type of machine learning algorithm that allows an agent to learn how to take actions in an environment to maximize a reward signal over time. In RL, an agent receives feedback in the form of rewards or penalties based on its actions. The agent's goal is to learn to take actions that lead to the highest possible reward.\n",
            "\n",
            "==================================================\n",
            "Question: What are some differences between reinforcement learning and supervised learning?\n",
            "\n",
            "Response: 1. Type of feedback: Reinforcement learning uses feedback in the form of rewards or penalties, while supervised learning uses feedback in the form of labels.\n",
            "\n",
            "2. Decision-making process: Reinforcement learning determines the optimal actions for a given state based on the current state and rewards, while supervised learning selects the optimal actions for a given state based on the training data.\n",
            "\n",
            "3. Learning algorithm: Reinforcement learning uses a policy gradient algorithm, while supervised learning uses a gradient descent algorithm.\n",
            "\n",
            "4. Training process: Reinforcement learning typically involves training an agent using a Monte Carlo algorithm, while supervised learning uses gradient descent.\n",
            "\n",
            "5. Model complexity: Reinforcement learning typically requires a large model, while supervised learning may require a small model.\n",
            "\n",
            "6. Interpretability: Reinforcement learning often provides interpretable models, while supervised learning often provides black-box models.\n",
            "\n",
            "7. Applications: Reinforcement learning is used in many applications such as gaming, robotics, and autonomous vehicles, while supervised learning is used in many applications such as classification, regression, and clustering.\n",
            "\n",
            "==================================================\n",
            "Question: What are some benefits of reinforcement learning over deep reinforcement learning?\n",
            "\n",
            "Response: Reinforcement learning (RL) and deep reinforcement learning (DRL) have distinct advantages and disadvantages. Here are some differences and benefits:\n",
            "\n",
            "Benefits of Reinforcement Learning:\n",
            "1. Less computation: RL is a meta-learning approach that uses less computation than deep learning.\n",
            "2. Better generalization: RL models can handle more complex and diverse environments, while deep learning models may struggle with certain tasks.\n",
            "3. Faster learning: RL models can learn faster than deep learning models.\n",
            "\n",
            "Disadvantages of Reinforcement Learning:\n",
            "1. Limited model flexibility: RL models are limited in their ability to handle complex tasks.\n",
            "2. Limited scalability: RL models are often limited by the size of the environment and the amount of memory available.\n",
            "3. Limited transferability: RL models are less transferable than deep learning models, as they may require significant training data.\n",
            "\n",
            "Benefits of Deep Reinforcement Learning:\n",
            "1. Scalability: DRL models can handle large datasets, making them suitable for use in large-scale applications.\n",
            "2. Transferability: DRL models can be used to transfer learning from one task to another\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Modified baseline RAG, prompt template giving more guidance\n",
        "def prompt_rag_response(query, index, embedding_model, llm_model, llm_tokenizer, index_to_doc_map, top_k=3):\n",
        "    # Create query embedding\n",
        "    query_embedding = embedding_model.encode([query]).astype(np.float32)\n",
        "\n",
        "    # Get top k docs\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    retrieved_docs = [index_to_doc_map[idx] for idx in indices[0]]\n",
        "\n",
        "    # Generate context from retrieved documents\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Create prompt for the model\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a Computer Science information retrieval assistant. Answer the question based on the provided context.\n",
        "If you don't know the answer based on the context, say \"I don't have enough information to answer this question.\"\n",
        "If you have enough information to give some context to the user, but not enough to make an exhaustive list, then simply give a short list that you are confident in.\n",
        "The questions will be about machine learning. Make sure you do not mix up concepts such as supervised learning, unsupervised learning and reinforcement learning, which are all different.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "<|user|>\n",
        "{query}\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    input_ids = llm_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(llm_model.device)\n",
        "\n",
        "    generation_config = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"do_sample\": True\n",
        "    }\n",
        "\n",
        "    # Generate the output\n",
        "    with torch.no_grad():\n",
        "        output = llm_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            **generation_config\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = llm_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the model's response\n",
        "    response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "for question in queries:\n",
        "    print(f\"Question: {question}\\n\")\n",
        "\n",
        "    response = prompt_rag_response(\n",
        "        query=question,\n",
        "        index=index,\n",
        "        embedding_model=embedding_model,\n",
        "        llm_model=model,\n",
        "        llm_tokenizer=tokenizer,\n",
        "        index_to_doc_map=index_doc_map,\n",
        "        top_k=5\n",
        "    )\n",
        "\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jj_6Ow2uU9X"
      },
      "source": [
        "The additional prompt information improved the resulting responses from the reinforcement learning questions considerably versus other methods so far. The description of RL is similar to other methods responses, and all of the methods so far has answered it correctly. The next 2 questions are typically more difficult for the methods explored so far. The second question contained some hallucinations, with the LLM stating that supervised learning selects actions, but overall did quite well. It even brought up gradient descent vs. Monte Carlo methods, and model interpretability, which are both valid points. Finally, the last question was answered quite well. The question itself is supposed to trap the LLM as Deep RL and RL each have their advantages and disadvantages. Despite this, the LLM was able to recognize this and list both the advantages and disadvantages of RL and Deep RL, without any major hallucinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-DlZR9Svgrm",
        "outputId": "1980271f-f19a-4d49-9100-87cb2db3616d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is reinforcement learning in machine learning?\n",
            "\n",
            "k = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Reinforcement learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions. In reinforcement learning, the agent's goal is to maximize the total reward it receives by taking actions that lead to the highest possible reward. In other words, the agent's goal is to maximize its expected cumulative reward.\n",
            "\n",
            "==================================================\n",
            "k = 5\n",
            "Response: Reinforcement learning (RL) is a machine learning technique that enables machines to learn and improve their behavior through interactions with the environment. In RL, an agent or machine receives feedback in the form of rewards or punishments, which it uses to update its policy or actions based on its current state and actions. This process is repeated continuously until the agent learns to perform optimally in the environment.\n",
            "\n",
            "==================================================\n",
            "k = 10\n",
            "Response: Reinforcement learning is a type of machine learning that is based on the concept of reward signaling. In reinforcement learning, an agent (i.e. A machine or a computer) is trained to learn to take actions in an environment to maximize a reward signal over time. The agent receives feedback in the form of rewards or penalties based on its actions. Reinforcement learning is a subset of artificial intelligence that focuses on developing algorithms and models that allow computers to learn patterns from data and make predictions or decisions.\n",
            "\n",
            "==================================================\n",
            "Question: What are some differences between reinforcement learning and supervised learning?\n",
            "\n",
            "k = 1\n",
            "Response: Sure! Here are some key differences between reinforcement learning and supervised learning:\n",
            "\n",
            "1. Reinforcement Learning:\n",
            "- Rewards are provided to the agent to encourage it to learn from its actions.\n",
            "- The agent learns to take actions to maximize a reward signal over time.\n",
            "- The agent is not directly programmed by humans, but rather by the environment or a supervisor.\n",
            "\n",
            "2. Supervised Learning:\n",
            "- The agent is programmed by humans or a supervisor to take actions to achieve a specific goal.\n",
            "- The agent is directly programmed by humans or a supervisor.\n",
            "- The goal is explicitly defined, which means that the agent must achieve a specific outcome to be successful.\n",
            "\n",
            "3. Reinforcement Learning vs. Supervised Learning:\n",
            "- Reinforcement learning learns from its own experiences, whereas supervised learning takes pre-programmed actions and evaluates the resulting outcome.\n",
            "- Reinforcement learning learns based on a reward signal, whereas supervised learning learns based on a specific outcome.\n",
            "- Reinforcement learning is often used to solve complex, non-linear problems, whereas supervised learning is typically used to solve simpler, linear problems\n",
            "\n",
            "==================================================\n",
            "k = 5\n",
            "Response: Sure, here are some differences between reinforcement learning and supervised learning:\n",
            "\n",
            "1. Learning objectives: Reinforcement learning is designed to learn a policy or strategy for making decisions, while supervised learning aims to learn a set of parameters or weights for a model.\n",
            "\n",
            "2. Data requirements: Reinforcement learning requires large amounts of labeled training data to learn, while supervised learning often relies on labeled data and is not as sensitive to the quality of the training data.\n",
            "\n",
            "3. Goal-directedness: Reinforcement learning is goal-directed, while supervised learning aims to maximize a specific objective.\n",
            "\n",
            "4. Learning rates: Reinforcement learning has lower learning rates than supervised learning, which allows for more exploration and adaptation during training.\n",
            "\n",
            "5. Interpretability: Reinforcement learning is typically interpretable and can explain how decisions were made based on the policy, while supervised learning is more black-box and may not be easy to interpret.\n",
            "\n",
            "6. Model complexity: Reinforcement learning models can be complex, while supervised learning models are typically simpler and easier to interpret.\n",
            "\n",
            "==================================================\n",
            "k = 10\n",
            "Response: Sure! Here are some differences between reinforcement learning and supervised learning:\n",
            "\n",
            "1. Data Requirement:\n",
            "Reinforcement learning is often used in domains where there is a lot of labeled data available, whereas supervised learning works better in situations where there is a small amount of labeled data available.\n",
            "\n",
            "2. Learning Strategy:\n",
            "In reinforcement learning, the agent has to learn from its own actions and decisions, whereas in supervised learning, the algorithm learns from labeled data.\n",
            "\n",
            "3. Learning Rate:\n",
            "Reinforcement learning uses a learning rate that is adjusted based on the current performance of the agent, whereas supervised learning uses a fixed learning rate.\n",
            "\n",
            "4. Learning Reward:\n",
            "In reinforcement learning, the reward signal is provided based on the current state of the environment, whereas in supervised learning, the reward signal is provided based on the performance of the model on new data.\n",
            "\n",
            "5. Robustness:\n",
            "Reinforcement learning is often more robust to noise and uncertainty in the environment than supervised learning, whereas supervised learning is more robust to noise and uncertainty in the data.\n",
            "\n",
            "6. Policy Grad\n",
            "\n",
            "==================================================\n",
            "Question: What are some benefits of reinforcement learning over deep reinforcement learning?\n",
            "\n",
            "k = 1\n",
            "Response: Reinforcement learning (RL) and deep reinforcement learning (DRL) are both types of artificial intelligence (AI) algorithms used for solving complex tasks. Here are some benefits of each over the other:\n",
            "\n",
            "1. RL:\n",
            "- Offers more flexibility in choosing the actions to take based on the state of the environment.\n",
            "- Can be used for tasks like playing video games, controlling robots, and learning from human behavior.\n",
            "- Can handle tasks with a high level of uncertainty, complexity, and variability.\n",
            "\n",
            "2. DRL:\n",
            "- Offers more control over the training process, allowing for more customization and flexibility.\n",
            "- Can be used for tasks like self-driving cars, playing games, and training agents to solve complex problems.\n",
            "- Can handle tasks with more regularity and consistency, reducing the need for human intervention.\n",
            "\n",
            "Benefits of Reinforcement Learning over Deep Reinforcement Learning:\n",
            "\n",
            "1. Flexibility: RL allows for more flexibility in choosing the actions to take based on the state of the environment, making it easier to handle tasks with complex or unpredictable outcomes.\n",
            "\n",
            "2. Control: D\n",
            "\n",
            "==================================================\n",
            "k = 5\n",
            "Response: Sure! Here are some benefits of reinforcement learning over deep reinforcement learning:\n",
            "\n",
            "1. Faster convergence: Reinforcement learning learns by interacting with the environment and achieving rewards. Deep learning, on the other hand, learns through backpropagation and requires more computation to converge.\n",
            "\n",
            "2. Limited data requirements: Deep learning models require a lot of training data to be able to perform well on new tasks. Reinforcement learning, on the other hand, can learn from limited data, such as simulated environments, without requiring a lot of training data.\n",
            "\n",
            "3. Better generalization: Reinforcement learning models are generally better at generalizing to new tasks than deep learning models. This is because reinforcement learning models can learn from a wide variety of environments and can adapt to new tasks by using the knowledge they have learned in the past.\n",
            "\n",
            "4. Improved performance: Reinforcement learning models can often achieve better performance than deep learning models, especially when dealing with complex tasks or when dealing with limited data.\n",
            "\n",
            "5. Lower risk of overfitting: Reinforcement learning models are more robust to overfitting, which is a common issue in deep learning models\n",
            "\n",
            "==================================================\n",
            "k = 10\n",
            "Response: Reinforcement learning (RL) has several benefits over deep reinforcement learning (DRL), which are:\n",
            "\n",
            "1. Faster convergence: Reinforcement learning converges more quickly than deep learning.\n",
            "\n",
            "2. Reduced data requirements: RL can learn from limited data and achieve better performance compared to DL.\n",
            "\n",
            "3. Improved generalization: RL can handle complex tasks that DL cannot, such as unseen scenarios or variations in environments.\n",
            "\n",
            "4. Better performance: RL can learn to solve complex problems faster and with better performance compared to DL.\n",
            "\n",
            "5. Better policy evaluation: RL can evaluate policies more efficiently, leading to better performance.\n",
            "\n",
            "6. Better policy synthesis: RL can generate policies more efficiently, leading to better performance.\n",
            "\n",
            "7. Better model generalization: RL can improve the generalization capabilities of DL models, leading to better performance.\n",
            "\n",
            "8. Better model interpretability: RL can provide more detailed explanations of DL models, leading to better interpretability.\n",
            "\n",
            "9. Better model parallelization: RL can parallelize training across multiple devices, leading to better performance.\n",
            "\n",
            "10. Better model robust\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Test different amounts of documents retrieved.\n",
        "for question in queries:\n",
        "    print(f\"Question: {question}\\n\")\n",
        "    for k in [1, 5, 10]:\n",
        "        print(f\"k = {k}\")\n",
        "\n",
        "        response = rag_response(\n",
        "            query=question,\n",
        "            index=index,\n",
        "            embedding_model=embedding_model,\n",
        "            llm_model=model,\n",
        "            llm_tokenizer=tokenizer,\n",
        "            index_to_doc_map=index_doc_map,\n",
        "            top_k=k\n",
        "        )\n",
        "\n",
        "        print(f\"Response: {response}\\n\")\n",
        "        print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11PlyO9-xFL9"
      },
      "source": [
        "When increasing the amount of documents retrieved, the model seems to improve to a point, and then becomes relatively nonsensical. When retrieving just 1 document, the LLM can answer simple questions like defining RL. But in the questions that ask for a deeper comparison, it can fall short. For example, it wasn't able to recognize that supervised learning was being referred to in the general context, instead of some sort of offline RL as supervised learning or some other hallucinations. As the document count increased, the comparative questions contained more information, but at 10 documents, the LLM seemed to be adding more information that was less relevant to the question. This was unhelpful and made for the response to be worse than the 5 document response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XFekVpXyFt6"
      },
      "source": [
        "In all, the baseline performance of the LLM, with 5 documents retrieved, gives a decent description for RL, and degrading performance in more complex comparative questions, and often includes hallucinations.\n",
        "\n",
        "The Multi Query technique performed better than the baseline, in that it seemed able to add more information to it's responses. This decreased hallucinations and added more context for the reader. The HyDE technique performed the best versus the MQ technique or the baseline. It seemed to gather more quality context to base it's response off of, and would hallucinate less than the baseline. It was also able to include valid points slightly more often than MQ.\n",
        "\n",
        "The prompt engineering technique, where more guidance was added to the LLM through the prompt, to make it more task specific, provided the largest jump in quality from the baseline. It was able to answer the RL description of course, but it was also able to answer the second question with only a few pain points. Finally, it was able to navigate the final query without falling into the trap that the query implies that RL or Deep RL are better than each other, when they are generally used in different cases or are involved with completely different methods.\n",
        "\n",
        "Finally, the effect of the number of documents retrieved was the least pronounced of the methods. This is likely because while more context is being provided, the documents will get less relevant as more are retrieved. I do believe that combining more documents being retrieved with a measure to ensure that the documents meet a similarity threshold would be useful, to avoid telling the LLM that some document is relevant when in reality, it is not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDmKBU_F1bfb"
      },
      "source": [
        "## Using a pretrained LLM for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf3l1kcT1AdL"
      },
      "source": [
        "The model will take in a text string and output a label and a score. The label indicates whether the LLM believes it is a positive or negative opinion. The score indicates how confident the LLM is in it's prediction.\n",
        "\n",
        "I am using the distilbert-base-uncased-finetuned-sst-2-english model which is a fine-tuned model from distilbert-base-uncased. This uses Supervised Fine Tuning (SFT) to tune the model to sentiment analysis. It is different from the TinyLlama LLM used in the previous code.\n",
        "\n",
        "Reference: https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m24DtliOuHM1",
        "outputId": "068f1595-30b8-4fd6-a304-4caf3d2e62eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: This course was incredibly insightful and well-structured.\n",
            "Predicted Sentiment: POSITIVE (Confidence: 0.9999)\n",
            "------------------------------------------------------------\n",
            "Text: I don't like the way this algorithm was explained.\n",
            "Predicted Sentiment: NEGATIVE (Confidence: 0.9983)\n",
            "------------------------------------------------------------\n",
            "Text: The model performance was excellent even on difficult examples.\n",
            "Predicted Sentiment: POSITIVE (Confidence: 0.9997)\n",
            "------------------------------------------------------------\n",
            "Text: This is the worst textbook I’ve ever used.\n",
            "Predicted Sentiment: NEGATIVE (Confidence: 0.9998)\n",
            "------------------------------------------------------------\n",
            "Text: It’s okay, but could use more examples and clarity.\n",
            "Predicted Sentiment: POSITIVE (Confidence: 0.8375)\n",
            "------------------------------------------------------------\n",
            "Text: Reinforcement Learning is my least favourite ML category.\n",
            "Predicted Sentiment: NEGATIVE (Confidence: 0.9996)\n",
            "------------------------------------------------------------\n",
            "Text: Reinforcement Learning is my favourite ML category.\n",
            "Predicted Sentiment: POSITIVE (Confidence: 0.9794)\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Hugging face LLM called distilbert, fined tuned using SFT to do sentiment analysis\n",
        "# This also grabs a sentiment analysis pipeline from HF that handles preprocessing and translating logits to label and confidence score.\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "sentences = [\n",
        "    \"This course was incredibly insightful and well-structured.\",\n",
        "    \"I don't like the way this algorithm was explained.\",\n",
        "    \"The model performance was excellent even on difficult examples.\",\n",
        "    \"This is the worst textbook I’ve ever used.\",\n",
        "    \"It’s okay, but could use more examples and clarity.\",\n",
        "    \"Reinforcement Learning is my least favourite ML category.\",\n",
        "    \"Reinforcement Learning is my favourite ML category.\"\n",
        "]\n",
        "\n",
        "# Sentiment analysis\n",
        "for text in sentences:\n",
        "    result = classifier(text)[0]\n",
        "    label = result['label']\n",
        "    score = result['score']\n",
        "    print(f\"Text: {text}\\nPredicted Sentiment: {label} (Confidence: {score:.4f})\\n{'-'*60}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
